{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "falling-alert",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [06:56<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import activations\n",
    "from tqdm import tqdm\n",
    "\n",
    "def NN(env):\n",
    "\n",
    "  input_size = env.observation_space.shape[0] + 1\n",
    "  output_size = env.action_space.n - 1\n",
    "  model = Sequential() \n",
    "  model.add(Dense(32, input_shape=(input_size,)))\n",
    "  model.add(Activation(activations.relu))\n",
    "  model.add(Dense(32, input_shape=(32,)))\n",
    "  model.add(Activation(activations.relu))\n",
    "  model.add(Dense(32, input_shape=(32,)))\n",
    "  model.add(Activation(activations.relu))\n",
    "  model.add(Dense(output_size))\n",
    "\n",
    "  return model\n",
    "\n",
    "def get_action(state, model, e):\n",
    "\n",
    "    state1 = np.pad(state, (0,1), mode='constant', constant_values=(0)).reshape(1,-1)\n",
    "    state2 = np.pad(state, (0,1), mode='constant', constant_values=(1)).reshape(1,-1)\n",
    "    return np.random.choice([np.argmax([model(state1)[0], model(state2)[0]]), 0, 1],\\\n",
    "                            p=[1 - e, e / 2. , e / 2.])\n",
    "\n",
    "def q_hat(model, state, action):\n",
    "    \n",
    "    inp = np.pad(state, (0,1), mode='constant', constant_values=(action)).reshape(1,-1)\n",
    "\n",
    "    return model(inp)\n",
    "\n",
    "# Based on algorithm 10.1\n",
    "def train(env, model, optimizer, loss, n_per_episode=100, n_episodes = 1024, e = 0.1):\n",
    "\n",
    "  for _ in tqdm(range(n_episodes)):\n",
    "    s = env.reset()\n",
    "    a = get_action(s, model, e)\n",
    "    for i in range(n_per_episode):\n",
    "      sn, reward, done, info = env.step(a)\n",
    "      if done:\n",
    "        with tf.GradientTape() as tape:\n",
    "          err = loss(reward, q_hat(model,s, a))\n",
    "        grads = tape.gradient(err, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        break\n",
    "      else:\n",
    "        an = get_action(sn, model, e)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "          err = loss(reward + q_hat(model, sn, an), q_hat(model, s, a))\n",
    "        grads = tape.gradient(err, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        s = sn\n",
    "        a = an\n",
    "  return model\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "model = NN(env)\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "agent = train(env, model, optimizer, loss, n_per_episode=100, n_episodes=1024, e=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparable-cleaner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:31<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Based on OpenAI specifications considered solved when the average reward is greater than or equal to\n",
    "#195.0 over 100 consecutive trials.\n",
    "\n",
    "avg_reward = 0\n",
    "max_num_episodes = 200\n",
    "trials = 100\n",
    "for episodes in tqdm(range(trials)):\n",
    "  obs = env.reset()\n",
    "  for reward in range(max_num_episodes):\n",
    "    action = get_action(obs, agent, e=0.0)\n",
    "    obs, reward, done, info = env.step(action) \n",
    "    avg_reward += reward\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "print(f'Average reward: {avg_reward / trials}')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
